{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T02:36:27.379723Z",
     "start_time": "2022-01-25T02:36:27.359724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용중인 파이썬 version은 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] 입니다.\n",
      "현재 사용중인 numpy version은 1.19.5 입니다.\n",
      "현재 사용중인 pandas version은 1.1.3 입니다.\n",
      "현재 사용중인 matplotlib version은 3.3.2 입니다.\n",
      "현재 사용중인 lgbm version은 3.1.1 입니다.\n"
     ]
    }
   ],
   "source": [
    "#로직 설정용 라이브러리 모음\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_row',500)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "from pandas import DataFrame\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "#그래프 생성 라이브러리 모음\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import font_manager, rc #그래프 폰트 깨질경우 방지 부분\n",
    "matplotlib.rc('font', family='NanumBarunGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "#폰트 깨지는거 수정 부분\n",
    "import platform\n",
    "if platform.system()=='Windows':\n",
    "    font_name=font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "    rc('font',family=font_name)\n",
    "else:\n",
    "    rc('font',family='AppleGothic')\n",
    "\n",
    "#머신러닝 수행 라이브러리 모음\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "import lightgbm as lgbm # lightgbm 부스팅 알고리즘 사용\n",
    "from lightgbm import LGBMClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "#현재 데이터에서 발생하는 type error를 해결하기 위한 함수 생성\n",
    "def coerce_df_columns_to_numeric(df, column_list):\n",
    "    df[column_list] = df[column_list].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    \n",
    "print(\"현재 사용중인 파이썬 version은 \" + sys.version + \" 입니다.\")\n",
    "print(\"현재 사용중인 numpy version은 \" + np.__version__ + \" 입니다.\")\n",
    "print(\"현재 사용중인 pandas version은 \" + pd.__version__ + \" 입니다.\")\n",
    "print(\"현재 사용중인 matplotlib version은 \" + matplotlib.__version__ + \" 입니다.\")\n",
    "print(\"현재 사용중인 lgbm version은 \" + lgbm.__version__ + \" 입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "## service 편집\n",
    "train_service = pd.read_csv(\"train_service.csv\")\n",
    "\n",
    "#user별 최신 등록일 표기(중복 가입 사례 제거)\n",
    "train_service_drop_use = train_service.sort_values(['user_id', 'reg_date'])['user_id'].drop_duplicates(keep='last')\n",
    "train_service2 = train_service.iloc[train_service_drop_use.index].reset_index(drop=True)\n",
    "\n",
    "# 미국 달러 결제자에 대한 환전 작업\n",
    "# dollar = []\n",
    "for i in range(train_service2.shape[0]):\n",
    "    if str(train_service2['pgamount'][i])[-1] == '9':\n",
    "#         dollar.append('dollar')\n",
    "        train_service2['pgamount'][i] = round(train_service2['pgamount'][i],2) * 1100\n",
    "    else:\n",
    "#         dollar.append('korean')\n",
    "\n",
    "# train_service2['dollar'] = dollar\n",
    "train_service2['pgamount'] = [round(i, -2) for i in train_service2['pgamount']] #소수점 제거\n",
    "\n",
    "# 20대 미만, 20대 ~ 50대, 60대 이상 나이 통합\n",
    "train_service2 = train_service2[train_service2['agegroup'] <= 100]\n",
    "train_service2['agegroup'] = [str(i) for i in train_service2['agegroup']]\n",
    "train_service2.loc[(train_service2['agegroup'] <= '15') |\n",
    "                   (train_service2['agegroup'] == '5'), 'agegroup'] = '20대 미만'\n",
    "train_service2.loc[(train_service2['agegroup'] == '20') | \n",
    "                   (train_service2['agegroup'] == '25'), 'agegroup'] = '20대'\n",
    "train_service2.loc[(train_service2['agegroup'] == '30') | \n",
    "                   (train_service2['agegroup'] == '35'), 'agegroup'] = '30대'\n",
    "train_service2.loc[(train_service2['agegroup'] == '40') | \n",
    "                   (train_service2['agegroup'] == '45'), 'agegroup'] = '40대'\n",
    "train_service2.loc[(train_service2['agegroup'] == '50') |\n",
    "                   (train_service2['agegroup'] == '55'), 'agegroup'] = '50대'\n",
    "train_service2.loc[(train_service2['agegroup'] >= '60'), 'agegroup'] = '60대 이상'\n",
    "\n",
    "\n",
    "# 가구내 인구수 3이면 4로 통합\n",
    "train_service2.loc[(train_service2['concurrentwatchcount'] == 3),\n",
    "                   'concurrentwatchcount'] = 4\n",
    "\n",
    "# 결제 코드 변환\n",
    "train_service2['chargetypeid'] = [str(i) for i in train_service2['chargetypeid']]\n",
    "train_service2.loc[(train_service2['chargetypeid'] == '121') |\n",
    "                   (train_service2['chargetypeid'] == '131'),\n",
    "                   'chargetypeid'] = '결제_휴대폰'\n",
    "train_service2.loc[(train_service2['chargetypeid'] == '132') |\n",
    "                   (train_service2['chargetypeid'] == '134'),\n",
    "                   'chargetypeid'] = '결제_신용카드'\n",
    "train_service2.loc[(train_service2['chargetypeid'] == '140'),\n",
    "                   'chargetypeid'] = '결제_아이튠즈'\n",
    "train_service2.loc[(train_service2['chargetypeid'] == '151'),\n",
    "                   'chargetypeid'] = '결제_원스토어'\n",
    "train_service2.loc[(train_service2['chargetypeid'] == '160') |\n",
    "                   (train_service2['chargetypeid'] == '170') |\n",
    "                   (train_service2['chargetypeid'] == '180') |\n",
    "                   (train_service2['chargetypeid'] == '190'),\n",
    "                   'chargetypeid'] = '결제_페이서비스'\n",
    "\n",
    "#등록 기기 타입 변환\n",
    "train_service2.loc[(train_service2['dev_typeid'] == 'mobile'),\n",
    "                   'dev_typeid'] = '결제기기_mobile_web'\n",
    "train_service2.loc[train_service2['dev_typeid'] == 'pc',\n",
    "                   'dev_typeid'] = '결제기기_pc'\n",
    "train_service2.loc[train_service2['dev_typeid'] == 'ios', \n",
    "                   'dev_typeid'] = '결제기기_ios'\n",
    "train_service2.loc[train_service2['dev_typeid'] == 'android',\n",
    "                   'dev_typeid'] = '결제기기_android'\n",
    "train_service2.loc[(train_service2['dev_typeid'] != 'movile web') &\n",
    "                   (train_service2['dev_typeid'] != 'android') &\n",
    "                   (train_service2['dev_typeid'] != 'ios') &\n",
    "                   (train_service2['dev_typeid'] != 'pc'),\n",
    "                   'dev_typeid'] = '결제기기_스마트_tv_관련'\n",
    "\n",
    "# user_id별 가장 오래된, 최신 시청 시간 확인 후, 갭 차이 변수화\n",
    "min_max_bookmark = train_bookmark.groupby(['user_id'\n",
    "                                           ])['dates'].agg(['max', 'min'\n",
    "                                                            ]).reset_index()\n",
    "min_max_bookmark['max'] = [\n",
    "    datetime.strptime(min_max_bookmark['max'][i], '%Y-%m-%d')\n",
    "    for i in range(min_max_bookmark.shape[0])\n",
    "]\n",
    "\n",
    "min_max_bookmark['min'] = [\n",
    "    datetime.strptime(min_max_bookmark['min'][i], '%Y-%m-%d')\n",
    "    for i in range(min_max_bookmark.shape[0])\n",
    "]\n",
    "\n",
    "min_max_bookmark['day_gap'] = [\n",
    "    int(\n",
    "        str(min_max_bookmark['max'][i] -\n",
    "            min_max_bookmark['min'][i]).split(' ')[0])\n",
    "    for i in range(min_max_bookmark.shape[0])\n",
    "]\n",
    "\n",
    "train_service2 = pd.merge(train_service2,\n",
    "                          min_max_bookmark[['user_id', 'day_gap']],\n",
    "                          how='left',\n",
    "                          on='user_id')\n",
    "train_service2.loc[train_service2.isnull()['day_gap'], 'day_gap'] = 0\n",
    "\n",
    "## bookmark 편집\n",
    "train_bookmark = pd.read_csv(\"train_bookmark.csv\")\n",
    "\n",
    "# 각 컨텐츠 별, 시청 기기 변환\n",
    "train_bookmark['dev_type'] = [str(i) for i in train_bookmark['dev_type']]\n",
    "\n",
    "train_bookmark.loc[(train_bookmark['dev_type'] == '1'), 'dev_type'] = 'pc'\n",
    "train_bookmark.loc[(train_bookmark['dev_type'] == '2') |\n",
    "                   (train_bookmark['dev_type'] == '3'), 'dev_type'] = 'android'\n",
    "train_bookmark.loc[(train_bookmark['dev_type'] == '4') |\n",
    "                   (train_bookmark['dev_type'] == '5'), 'dev_type'] = 'ios'\n",
    "train_bookmark.loc[(train_bookmark['dev_type'] != 'pc') &\n",
    "                   (train_bookmark['dev_type'] != 'android') &\n",
    "                   (train_bookmark['dev_type'] != 'ios'),\n",
    "                   'dev_type'] = '스마트 tv 관련'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공통 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 코인 이벤트 관련 데이터\n",
    "copper = pd.read_csv(\"copper.csv\")\n",
    "copper = copper.sort_values(['user_id','reg_date']).reset_index(drop=True)\n",
    "copper['co'] = [copper['prod_code'][i][0:2] for i in range(copper.shape[0])]\n",
    "\n",
    "abc = DataFrame(Counter(copper['prod_code']).values(), Counter(copper['prod_code']).keys()).reset_index()\n",
    "abc['ab'] = [abc['index'][i][0:2] for i in range(abc.shape[0])]\n",
    "\n",
    "#컨텐츠 관련 데이터\n",
    "컨텐츠_설명 = pd.read_csv(\"컨텐츠_설명.csv\")\n",
    "컨텐츠_설명 = 컨텐츠_설명[['cont_id', 'playtime', 'broaddate']]\n",
    "\n",
    "영화_설명 = pd.read_csv(\"영화_설명.csv\")\n",
    "영화_설명.columns = [\n",
    "    'cont_id', 'title_ko', 'title_en', 'region', 'director', 'actor',\n",
    "    'targetage', '설명', 'run_time'\n",
    "]\n",
    "영화_설명 = 영화_설명[['cont_id', 'region', 'targetage', 'run_time']]\n",
    "\n",
    "abc = []\n",
    "for i in range(영화_설명.shape[0]):\n",
    "    try:\n",
    "        if 영화_설명['run_time'][i][-1] == '분':\n",
    "            abc.append(영화_설명['run_time'][i][:-1])\n",
    "        else:\n",
    "            abc.append(영화_설명['run_time'][i])\n",
    "    except:\n",
    "        abc.append(영화_설명['run_time'][i])\n",
    "\n",
    "영화_설명['run_time'] = [float(i) * 60 for i in abc]\n",
    "\n",
    "## (통합버전)각 시청 기록에, content, movie 관련 정보 합치기\n",
    "train_content_bookmark = pd.merge(train_bookmark,\n",
    "                                  컨텐츠_설명,\n",
    "                                  on='cont_id',\n",
    "                                  how='left')\n",
    "train_total_bookmark = pd.merge(train_content_bookmark,\n",
    "                                영화_설명,\n",
    "                                on='cont_id',\n",
    "                                how='left')\n",
    "\n",
    "coerce_df_columns_to_numeric(train_total_bookmark, 'playtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "#드라마 playtime, 영화 run_time 합치기\n",
    "\n",
    "list1 = []\n",
    "for i in tqdm(range(train_total_bookmark.shape[0])):\n",
    "    if np.isnan(train_total_bookmark['playtime'][i]) == True:\n",
    "        list1.append(train_total_bookmark['run_time'][i])\n",
    "    else:\n",
    "        list1.append(train_total_bookmark['playtime'][i])\n",
    "\n",
    "#각 컨텐츠별 시간과 개인의 시청별 시간으로 view_percent를 추가 생성\n",
    "train_total_bookmark['content_time'] = list1\n",
    "train_total_bookmark.loc[train_total_bookmark['content_time'] == 0,\n",
    "                         ['content_time']] = np.nan\n",
    "train_total_bookmark['view_percent'] = train_total_bookmark[\n",
    "    'viewtime'] / train_total_bookmark['content_time']\n",
    "\n",
    "train_total_bookmark2 = train_total_bookmark.copy()\n",
    "train_total_bookmark2.loc[train_total_bookmark2['view_percent'] >= 1,\n",
    "                          ['view_percent']] = 1.00\n",
    "\n",
    "train_total_bookmark2.loc[train_total_bookmark2['channeltype'] == 'E',\n",
    "                          'channeltype'] = 'L'\n",
    "\n",
    "# 드라마, 영화 공개일과 시청일간의 차이를 계산 후, 변수 추가함\n",
    "drama_view_gap = []\n",
    "for i in tqdm(range(train_total_bookmark2.shape[0])):\n",
    "    try:\n",
    "        drama_view_gap.append(\n",
    "            (datetime.strptime(train_total_bookmark2['dates'][i], '%Y-%m-%d') -\n",
    "             datetime.strptime(train_total_bookmark2['broaddate'][i],\n",
    "                               '%Y-%m-%d')).days)\n",
    "    except:\n",
    "        drama_view_gap.append('해당 없음')\n",
    "\n",
    "train_total_bookmark2['drama_view_day_gap'] = drama_view_gap\n",
    "train_total_bookmark2.loc[train_total_bookmark2['drama_view_day_gap'] ==\n",
    "                          '해당 없음', 'drama_view_day_gap'] = -9999\n",
    "\n",
    "# 7일 이하를 최신 기간, 1년 미만을 중간 기간, 1년 이상과 영화(해당 없음)으로 추가 그룹화 수행\n",
    "temp1 = train_total_bookmark2[train_total_bookmark2['drama_view_day_gap'] ==\n",
    "                              -9999]\n",
    "temp1['drama_view_day_gap'] = '해당 없음'\n",
    "\n",
    "temp2 = train_total_bookmark2[\n",
    "    train_total_bookmark2['drama_view_day_gap'] >= 365]\n",
    "temp2['drama_view_day_gap'] = '1년 이상'\n",
    "\n",
    "temp3 = train_total_bookmark2[\n",
    "    (train_total_bookmark2['drama_view_day_gap'] > 7)\n",
    "    & (train_total_bookmark2['drama_view_day_gap'] < 365)]\n",
    "temp3['drama_view_day_gap'] = '중간 기간'\n",
    "\n",
    "temp4 = train_total_bookmark2[\n",
    "    (train_total_bookmark2['drama_view_day_gap'] >= 0)\n",
    "    & (train_total_bookmark2['drama_view_day_gap'] <= 7)]\n",
    "temp4['drama_view_day_gap'] = '최신 기간'\n",
    "\n",
    "train_total_bookmark3 = pd.concat([temp1, temp2, temp3, temp4],\n",
    "                                  axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "#채널 타입과 user별 시청수 결합 후, 특정 이상치 절삭\n",
    "DataFrame(\n",
    "    train_total_bookmark3.groupby([\n",
    "        'user_id', 'channeltype'\n",
    "    ])['dates'].count()).unstack().reset_index().to_csv('temp_channeltype.csv')\n",
    "\n",
    "content_count = pd.read_csv('temp_channeltype.csv', header=1)\n",
    "del content_count['channeltype']\n",
    "content_count.columns = ['user_id', 'ETC', 'MOVIE', 'VIEW_DRAMA']\n",
    "\n",
    "content_count.loc[content_count['VIEW_DRAMA'] >= 400, 'VIEW_DRAMA'] = 400\n",
    "content_count.loc[content_count['MOVIE'] >= 40, 'MOVIE'] = 40\n",
    "content_count.loc[content_count['ETC'] >= 40, 'ETC'] = 40\n",
    "content_count = content_count.fillna(0)\n",
    "\n",
    "train_total = pd.merge(train_service2, content_count, how='left', on='user_id')\n",
    "\n",
    "#드라마 시청 갭과 user별 시청수 결합 후, 특정 이상치 절삭\n",
    "DataFrame(\n",
    "    train_total_bookmark3.groupby([\n",
    "        'user_id', 'drama_view_day_gap'\n",
    "    ])['dates'].count()).unstack().reset_index().to_csv(\n",
    "        'temp_drama_view_day_gap.csv')\n",
    "\n",
    "drama_view_day_gap = pd.read_csv('temp_drama_view_day_gap.csv', header=1)\n",
    "del drama_view_day_gap['drama_view_day_gap']\n",
    "drama_view_day_gap.columns = ['user_id'] + list(drama_view_day_gap.columns[1:])\n",
    "drama_view_day_gap = drama_view_day_gap.fillna(0)\n",
    "drama_view_day_gap.loc[drama_view_day_gap['1년 이상'] >= 100, '1년 이상'] = 100\n",
    "drama_view_day_gap.loc[drama_view_day_gap['중간 기간'] >= 100, '중간 기간'] = 100\n",
    "drama_view_day_gap.loc[drama_view_day_gap['최신 기간'] >= 100, '최신 기간'] = 100\n",
    "drama_view_day_gap.loc[drama_view_day_gap['해당 없음'] >= 100, '해당 없음'] = 100\n",
    "\n",
    "train_total22 = pd.merge(train_total,\n",
    "                         drama_view_day_gap,\n",
    "                         how='left',\n",
    "                         on='user_id')\n",
    "\n",
    "#유저별, 평균 시청 퍼센트 계산\n",
    "view_percent = DataFrame(\n",
    "    train_total_bookmark3.groupby(['user_id'\n",
    "                                   ])['view_percent'].mean()).reset_index()\n",
    "\n",
    "train_total2 = pd.merge(train_total22, view_percent, how='left', on='user_id')\n",
    "train_total2.loc[train_total2.isnull()['view_percent'], ['view_percent']] = 0\n",
    "\n",
    "# 시각화 부문 미 실행 처리\n",
    "# plt.hist(train_total2['view_percent'],bins=50);\n",
    "# plt.hist(train_total2['view_count'],bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test data\n",
    "\n",
    "(train과 동일한 구조로 코드 생략 처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 재 구독율 관련 counter 계산 및 추가 시각화\n",
    "\n",
    "(데이터 미 공개로 전체 삭제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링 전 데이터 최종 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "# 전 처리된 데이터 불러오기\n",
    "\n",
    "# train = pd.read_csv(\"train_eda_ver9.csv\")\n",
    "train_set2 = train_total2.copy()\n",
    "\n",
    "#user_id, 날짜, 추가 미사용 변수 제거\n",
    "del train_set2['user_id']\n",
    "del train_set2['reg_date']\n",
    "del train_set2['enddate']\n",
    "del train_set2['prod_code']\n",
    "del train_set2['promo_100']\n",
    "\n",
    "# 반응 변수 추가 제거\n",
    "del train_set2['Repurchase']\n",
    "\n",
    "#train_y(반응변수만 있는 데이터) 생성\n",
    "train_label = pd.get_dummies(train_set2['Repurchase'])\n",
    "train_label = train_label['X']\n",
    "\n",
    "#category 변수 dummy화 선언(train)\n",
    "chargetypeid_dummy = pd.get_dummies(train_set2['chargetypeid'])\n",
    "del train_set2['chargetypeid']\n",
    "\n",
    "copperReceived_dummy = pd.get_dummies(train_set2['copperReceived'],\n",
    "                                      dummy_na=True)\n",
    "copperReceived_dummy.columns = ['copperReceived_yes', 'copperReceived_no']\n",
    "del copperReceived_dummy['copperReceived_no']\n",
    "del train_set2['copperReceived']\n",
    "\n",
    "dev_typeid_dummy = pd.get_dummies(train_set2['dev_typeid'], dummy_na=True)\n",
    "del train_set2['dev_typeid']\n",
    "\n",
    "isauth_dummy = pd.get_dummies(train_set2['isauth'], dummy_na=True)\n",
    "isauth_dummy.columns = ['isauth_yes', 'isauth_no']\n",
    "del isauth_dummy['isauth_no']\n",
    "del train_set2['isauth']\n",
    "\n",
    "gender_dummy = pd.get_dummies(train_set2['gender'], dummy_na=True)\n",
    "gender_dummy.columns = ['F', 'M', '성별 모름']\n",
    "del train_set2['gender']\n",
    "\n",
    "agegroup_dummy = pd.get_dummies(train_set2['agegroup'])\n",
    "del train_set2['agegroup']\n",
    "\n",
    "train_set = pd.concat([\n",
    "    train_set2, chargetypeid_dummy, copperReceived_dummy, dev_typeid_dummy,\n",
    "    isauth_dummy, gender_dummy, agegroup_dummy\n",
    "], axis=1)\n",
    "\n",
    "#category 변수 dummy화 선언(test) 생략함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "source": [
    "## using gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_BOOST_ROUND = 10000\n",
    "n_splits =5\n",
    "SEED = 1993\n",
    "\n",
    "lgbm_param = {'objective':'binary',\n",
    "              'boosting_type': 'gbdt',\n",
    "              'random_state':1993,\n",
    "              'learning_rate':0.01,\n",
    "              'subsample':0.8,\n",
    "            \"metric\" : \"auc\",\n",
    "            'subsample_freq': 1,\n",
    "              'reg_lambda':0.1,\n",
    "              'reg_alpha': 0.2,\n",
    "               'num_leaves': (2**4)-1,\n",
    "              'colsample_bytree':0.9,\n",
    "            }\n",
    "\n",
    "cv_list = []\n",
    "evals_result = {}\n",
    "real_train_f1, real_valid_f1 = [], []\n",
    "\n",
    "kfolds = StratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "\n",
    "cv_subsample = [0.8,0.9]\n",
    "cv_reg_lambda = [0.1,0.2]\n",
    "cv_reg_alpha = [0.2,0.4]\n",
    "cv_num_leaves = [8,16]\n",
    "cv_colsample_bytree = [0.8,0.9]\n",
    "\n",
    "total_items2 = [cv_subsample,cv_reg_lambda,cv_reg_alpha,\n",
    "              cv_num_leaves,cv_colsample_bytree]\n",
    "\n",
    "total_items = list(product(*total_items2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "for jk in tqdm_notebook(range(len(total_items))):\n",
    "    one_items = total_items[jk]\n",
    "    \n",
    "    lgbm_param['subsample'] = one_items[0]\n",
    "    lgbm_param['reg_lambda'] = one_items[1]\n",
    "    lgbm_param['reg_alpha'] = one_items[2]\n",
    "    lgbm_param['num_leaves'] = one_items[3]\n",
    "    lgbm_param['colsample_bytree'] = one_items[4] \n",
    "    \n",
    "    train_f1, valid_f1 = [], []\n",
    "\n",
    "    for ind, (trn_ind, val_ind) in tqdm_notebook( enumerate(kfolds.split(X= train_set, y = train_label)) ):\n",
    "\n",
    "        X_train, y_train = train_set.iloc[trn_ind], train_label[trn_ind]\n",
    "        X_valid, y_valid = train_set.iloc[val_ind], train_label[val_ind]\n",
    "\n",
    "        dtrain = lgbm.Dataset( X_train, y_train )\n",
    "        dvalid = lgbm.Dataset( X_valid, y_valid ,reference=dtrain)\n",
    "\n",
    "        model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, valid_sets=(dtrain, dvalid), valid_names=('train','valid'), \n",
    "                            verbose_eval= 100, evals_result=evals_result, early_stopping_rounds=50)\n",
    "\n",
    "        train_x_predict = model.predict(X_train)\n",
    "        train_x_predict_binary = [1 if i>=0.29 else 0 for i in train_x_predict]\n",
    "        train_f1.append(f1_score(y_train, train_x_predict_binary))\n",
    "\n",
    "        valid_x_predict = model.predict(X_valid)\n",
    "        valid_x_predict_binary = [1 if i>=0.29 else 0 for i in valid_x_predict]\n",
    "        valid_f1.append(f1_score(y_valid, valid_x_predict_binary))\n",
    "\n",
    "        print('='*80)\n",
    "    \n",
    "    real_train_f1.append(np.mean(train_f1))\n",
    "    real_valid_f1.append(np.mean(valid_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "checking_value = DataFrame(total_items)\n",
    "checking_value.columns = ['subsample','reg_lambda','reg_alpha',\n",
    "                  'num_leaves','colsample_bytree']\n",
    "checking_value['train_f1'] = real_train_f1\n",
    "checking_value['valid_f1'] = real_valid_f1\n",
    "# checking_value.sort_values('valid_f1',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search 결과로 5-fold + threshold 찾기 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "lgbm_param = {'objective':'binary',\n",
    "              'boosting_type': 'gbdt',\n",
    "              'random_state':1993,\n",
    "              'learning_rate':0.01,\n",
    "              'subsample':0.9,\n",
    "            \"metric\" : \"auc\",\n",
    "            'subsample_freq': 1,\n",
    "              'reg_lambda':0.1,\n",
    "              'reg_alpha': 0.2,\n",
    "               'num_leaves': 8,\n",
    "              'colsample_bytree':0.9\n",
    "            }\n",
    "\n",
    "cv_list = []\n",
    "evals_result = {}\n",
    "train_f1 = []\n",
    "valid_f1 = []\n",
    "final_test = np.zeros( test.shape[0] )\n",
    "\n",
    "kfolds = StratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "\n",
    "for ind, (trn_ind, val_ind) in tqdm_notebook( enumerate(kfolds.split(X= train_set, y = train_label)) ):\n",
    "    \n",
    "    X_train, y_train = train_set.iloc[trn_ind], train_label[trn_ind]\n",
    "    X_valid, y_valid = train_set.iloc[val_ind], train_label[val_ind]\n",
    "    \n",
    "    dtrain = lgbm.Dataset( X_train, y_train )\n",
    "    dvalid = lgbm.Dataset( X_valid, y_valid ,reference=dtrain)\n",
    "    \n",
    "    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, valid_sets=(dtrain, dvalid), valid_names=('train','valid'), \n",
    "                        verbose_eval= 100, evals_result=evals_result, early_stopping_rounds=50)\n",
    "    \n",
    "    train_x_predict = model.predict(X_train)\n",
    "    train_x_predict_binary = [1 if i>=0.29 else 0 for i in train_x_predict]\n",
    "    train_f1.append(f1_score(y_train, train_x_predict_binary))\n",
    "\n",
    "    valid_x_predict = model.predict(X_valid)\n",
    "    valid_x_predict_binary = [1 if i>=0.29 else 0 for i in valid_x_predict]\n",
    "    valid_f1.append(f1_score(y_valid, valid_x_predict_binary))\n",
    "    \n",
    "    test_pred  = model.predict(test_set)\n",
    "    final_test += test_pred\n",
    "\n",
    "    print('='*80)\n",
    "    \n",
    "# np.mean(train_f1), np.mean(valid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "thres_train, thres_valid = [], []\n",
    "\n",
    "for thres in tqdm([round(i,3) for i in list(np.arange(0.01, 0.99, 0.005))]):\n",
    "    train_x_predict_binary = [1 if i>=thres else 0 for i in train_x_predict]\n",
    "    thres_train.append(f1_score(y_train, train_x_predict_binary))\n",
    "    valid_x_predict_binary = [1 if i>=thres else 0 for i in valid_x_predict]\n",
    "    thres_valid.append(f1_score(y_valid, valid_x_predict_binary))\n",
    "\n",
    "checking = DataFrame([[round(i,3) for i in list(np.arange(0.01, 0.99, 0.005))], thres_train, thres_valid]).T\n",
    "checking.columns = ['threshold','train_f1','valid_f1']\n",
    "# checking.sort_values('valid_f1',ascending=False).head(10)\n",
    "\n",
    "best_threshold = float(checking.sort_values('valid_f1',ascending=False)['threshold'].head(1))\n",
    "\n",
    "train_x_predict = model.predict(X_train)\n",
    "train_x_predict_binary = [1 if i>= best_threshold else 0 for i in train_x_predict]\n",
    "\n",
    "# print(Counter(train_x_predict_binary), Counter(y_train))\n",
    "# accuracy_score(y_train, train_x_predict_binary), f1_score(y_train, train_x_predict_binary)\n",
    "\n",
    "valid_x_predict = model.predict(X_valid)\n",
    "valid_x_predict_binary = [1 if i>= best_threshold else 0 for i in valid_x_predict]\n",
    "\n",
    "# print(Counter(valid_x_predict_binary), Counter(y_valid))\n",
    "# accuracy_score(y_valid, valid_x_predict_binary),f1_score(y_valid, valid_x_predict_binary)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16, 12))\n",
    "# lgbm.plot_importance(model, ax=ax)\n",
    "\n",
    "# confusion_matrix(y_train, train_x_predict_binary)\n",
    "\n",
    "# print(classification_report(y_train, train_x_predict_binary))\n",
    "# print(classification_report(y_valid, valid_x_predict_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "real_final = final_test / 5\n",
    "test_predict = [1 if i>=best_threshold else 0 for i in real_final]\n",
    "\n",
    "test = pd.read_csv(\"test_eda_ver7.csv\")\n",
    "test['value'] = test_predict\n",
    "\n",
    "submission = pd.read_csv(\"C:\\\\Users\\\\bluemumin\\\\Desktop\\\\CDS\\\\Predict\\\\CDS_submission.csv\")\n",
    "submission['user_id'] = [i.split('|')[0] for i in submission['KEY']]\n",
    "submission.head()\n",
    "\n",
    "real_submission = pd.merge(submission, test[['user_id','value']], how='left', on='user_id', indicator=True)\n",
    "\n",
    "real_submission2 = real_submission[['KEY','value']]\n",
    "real_submission2.columns = ['KEY','CHURN']\n",
    "\n",
    "real_submission2.to_csv(\"wavve_재가입여부_분류결과.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 적용 후 취소 항목"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2-1 공통 데이터 중 결제 관련 데이터\n",
    "\n",
    "# product_code2 = pd.read_csv(\"자동결제.csv\")\n",
    "\n",
    "# check_auto = []\n",
    "# for i in range(product_code2.shape[0]):\n",
    "#     try:\n",
    "#         if product_code2['설명'][i].find('자동결제') >= 0:\n",
    "#             check_auto.append('자동결제')\n",
    "#         elif product_code2['설명'][i].find('미사용') >= 0:\n",
    "#             check_auto.append('미사용')\n",
    "#         else:\n",
    "#             check_auto.append('수동결제')\n",
    "#     except:\n",
    "#         check_auto.append('미사용')\n",
    "# product_code2['check_auto'] = check_auto\n",
    "\n",
    "# mobile_auto = []\n",
    "# for i in range(product_code2.shape[0]):\n",
    "#     try:\n",
    "#         if product_code2['설명'][i].find('모바일') >= 0:\n",
    "#             mobile_auto.append('모바일 가능')\n",
    "#         else:\n",
    "#             mobile_auto.append('없음')\n",
    "#     except:\n",
    "#         mobile_auto.append('없음')\n",
    "# product_code2['mobile_auto'] = mobile_auto\n",
    "\n",
    "# pc_auto = []\n",
    "# for i in range(product_code2.shape[0]):\n",
    "#     try:\n",
    "#         if product_code2['설명'][i].find('PC') >= 0:\n",
    "#             pc_auto.append('PC 가능')\n",
    "#         else:\n",
    "#             pc_auto.append('없음')\n",
    "#     except:\n",
    "#         pc_auto.append('없음')\n",
    "# product_code2['pc_auto'] = pc_auto\n",
    "\n",
    "# tv_auto = []\n",
    "# for i in range(product_code2.shape[0]):\n",
    "#     try:\n",
    "#         if product_code2['설명'][i].find('스마트TV') >= 0:\n",
    "#             tv_auto.append('스마트TV 가능')\n",
    "#         else:\n",
    "#             tv_auto.append('없음')\n",
    "#     except:\n",
    "#         tv_auto.append('없음')\n",
    "# product_code2['tv_auto'] = tv_auto\n",
    "\n",
    "# all_auto = []\n",
    "# for i in range(product_code2.shape[0]):\n",
    "#     try:\n",
    "#         if product_code2['설명'][i].find('전체 디바이스') >= 0:\n",
    "#             all_auto.append('전체 가능')\n",
    "#         else:\n",
    "#             all_auto.append('없음')\n",
    "#     except:\n",
    "#         all_auto.append('없음')\n",
    "# product_code2['all_auto'] = all_auto\n",
    "\n",
    "# product_code2.loc[product_code2['all_auto'] != '없음', 'mobile_auto'] = '모바일 가능'\n",
    "# product_code2.loc[product_code2['all_auto'] != '없음', 'pc_auto'] = 'PC 가능'\n",
    "# product_code2.loc[product_code2['all_auto'] != '없음', 'tv_auto'] = '스마트TV 가능'\n",
    "# del product_code2['all_auto']\n",
    "\n",
    "# hd_auto = []\n",
    "# for i in range(product_code2.shape[0]):\n",
    "#     try:\n",
    "#         if product_code2['설명'][i].find(' HD 화질') >= 0:\n",
    "#             hd_auto.append('HD 가능')\n",
    "#         else:\n",
    "#             hd_auto.append('없음')\n",
    "#     except:\n",
    "#         hd_auto.append('없음')\n",
    "# product_code2['hd_auto'] = hd_auto\n",
    "\n",
    "# fhd_auto = []\n",
    "# for i in range(product_code2.shape[0]):\n",
    "#     try:\n",
    "#         if product_code2['설명'][i].find('FHD 화질') >= 0:\n",
    "#             fhd_auto.append('FHD 가능')\n",
    "#         else:\n",
    "#             fhd_auto.append('없음')\n",
    "#     except:\n",
    "#         fhd_auto.append('없음')\n",
    "# product_code2['fhd_auto'] = fhd_auto\n",
    "\n",
    "# good_auto = []\n",
    "# for i in range(product_code2.shape[0]):\n",
    "#     try:\n",
    "#         if product_code2['설명'][i].find('최상위 화질') >= 0:\n",
    "#             good_auto.append('최상위 가능')\n",
    "#         else:\n",
    "#             good_auto.append('없음')\n",
    "#     except:\n",
    "#         good_auto.append('없음')\n",
    "# product_code2['good_auto'] = good_auto\n",
    "\n",
    "# product_code2.columns = ['prod_code', 'name', '설명'] + list( product_code2.columns[3:])\n",
    "# product_code = product_code2.copy()\n",
    "# del product_code['name']\n",
    "# del product_code['설명']\n",
    "\n",
    "# train_service2 = pd.merge(train_service2,\n",
    "#                           product_code,\n",
    "#                           how='left',\n",
    "#                           on='prod_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2 train data 시청 시간대\n",
    "#bookmark_device = train_bookmark.groupby(['user_id', 'dev_type'\n",
    "#                                           ])['dates'].count().reset_index()\n",
    "# bookmark_device2 = pd.pivot_table(bookmark_device,\n",
    "#                                   index=['user_id'],\n",
    "#                                   values='dates',\n",
    "#                                   columns='dev_type').reset_index()\n",
    "# bookmark_device2 = bookmark_device2.fillna(0)\n",
    "# bookmark_device2.columns = ['user_id', 'view_android', 'view_ios', 'view_pc', 'view_smart_tv']\n",
    "\n",
    "# bookmark_device2['best'] = bookmark_device2[list(\n",
    "#     bookmark_device2.columns[1:])].idxmax(axis=1)\n",
    "\n",
    "# train_bookmark.loc[(train_bookmark['sector'] == '1') |\n",
    "#                    (train_bookmark['sector'] == 'M01'), 'sector'] = '드라마관련'\n",
    "# train_bookmark.loc[(train_bookmark['sector'] == '2') |\n",
    "#                    (train_bookmark['sector'] == 'M03'), 'sector'] = '예능관련'\n",
    "# train_bookmark.loc[(train_bookmark['sector'] == '3'), 'sector'] = '시사관련'\n",
    "# train_bookmark.loc[(train_bookmark['sector'] == '8') |\n",
    "#                    (train_bookmark['sector'] == '6') |\n",
    "#                    (train_bookmark['sector'] == 'M07'), 'sector'] = '애니관련'\n",
    "# train_bookmark.loc[(train_bookmark['sector'] == '9') |\n",
    "#                    (train_bookmark['sector'] == '10'), 'sector'] = '해외관련'\n",
    "# train_bookmark.loc[(train_bookmark['sector'] == 'M02'), 'sector'] = '로맨스관련'\n",
    "# train_bookmark.loc[(train_bookmark['sector'] == 'M04') |\n",
    "#                    (train_bookmark['sector'] == 'M05') |\n",
    "#                    (train_bookmark['sector'] == 'M06'), 'sector'] = '스릴러관련'\n",
    "# train_bookmark.loc[(train_bookmark['sector'] == '4') |\n",
    "#                    (train_bookmark['sector'] == '5') |\n",
    "#                    (train_bookmark['sector'] == '7') |\n",
    "#                    (train_bookmark['sector'] == '11') |\n",
    "#                    (train_bookmark['sector'] == '12') |\n",
    "#                    (train_bookmark['sector'] == 'M08') |\n",
    "#                    (train_bookmark['sector'] == 'M90'), 'sector'] = '기타관련'\n",
    "# train_bookmark.loc[train_bookmark.isnull()['sector'], 'sector'] = '확인불가sector'\n",
    "\n",
    "# bookmark_sector = train_bookmark.groupby(['user_id', 'sector'\n",
    "#                                           ])['dates'].count().reset_index()\n",
    "# bookmark_sector2 = pd.pivot_table(bookmark_sector,\n",
    "#                                   index=['user_id'],\n",
    "#                                   values='dates',\n",
    "#                                   columns='sector').reset_index()\n",
    "# bookmark_sector2 = bookmark_sector2.fillna(0)\n",
    "# bookmark_sector2['sector_best'] = bookmark_sector2[list(\n",
    "#     bookmark_sector2.columns[1:])].idxmax(axis=1)\n",
    "\n",
    "\n",
    "# time_bookmark = train_bookmark.groupby(['user_id'])['hour'].agg(['mean']).reset_index()\n",
    "# time_bookmark.columns = ['user_id', '시청시간_mean']\n",
    "# time_bookmark['시청시간_mean'] = [round(i, 0) for i in time_bookmark['시청시간_mean']]\n",
    "\n",
    "\n",
    "# train_service2 = pd.merge(train_service2,\n",
    "#                           time_bookmark[['user_id', '시청시간_mean']],\n",
    "#                           how='left',\n",
    "#                           on='user_id')\n",
    "# train_service2.loc[train_service2.isnull()['시청시간_mean'], '시청시간_mean'] = 0\n",
    "\n",
    "# train_service2.loc[(train_service2['시청시간_mean'] >= 22) |\n",
    "#                    (train_service2['시청시간_mean'] <= 7), '시청시간_mean'] = -99\n",
    "# train_service2.loc[(train_service2['시청시간_mean'] >= 8) &\n",
    "#                    (train_service2['시청시간_mean'] <= 12), '시청시간_mean'] = 9\n",
    "# train_service2.loc[(train_service2['시청시간_mean'] >= 13) &\n",
    "#                    (train_service2['시청시간_mean'] <= 17), '시청시간_mean'] = 15\n",
    "# train_service2.loc[(train_service2['시청시간_mean'] >= 18) &\n",
    "#                    (train_service2['시청시간_mean'] <= 21), '시청시간_mean'] = 20\n",
    "\n",
    "# train_service2.loc[(train_service2['시청시간_mean'] == -99), '시청시간_mean'] = \"심야시간\"\n",
    "# train_service2.loc[(train_service2['시청시간_mean'] == 9), '시청시간_mean'] = \"아침시간\"\n",
    "# train_service2.loc[(train_service2['시청시간_mean'] == 15), '시청시간_mean'] = \"점심시간\"\n",
    "# train_service2.loc[(train_service2['시청시간_mean'] == 20), '시청시간_mean'] = \"저녁시간\"\n",
    "\n",
    "\n",
    "# train_total3 = pd.merge(train_total2,\n",
    "#                         bookmark_device2[['user_id', 'best']],\n",
    "#                         how='left',\n",
    "#                         on='user_id')\n",
    "# train_total3.loc[train_total3.isnull()['best'], 'best'] = 'not watch'\n",
    "\n",
    "# train_total4 = pd.merge(train_total3,\n",
    "#                         bookmark_sector2[['user_id', 'sector_best']],\n",
    "#                         how='left',\n",
    "#                         on='user_id')\n",
    "# train_total4.loc[train_total4.isnull()['sector_best'],\n",
    "#                  'sector_best'] = '미시청sector'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "source": [
    "## rf시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_set = train_set.fillna(0)\n",
    "# test_set = test_set.fillna(0)\n",
    "\n",
    "# NUM_BOOST_ROUND = 10000\n",
    "# n_splits =5\n",
    "# SEED = 1993\n",
    "# threshold = 0.28\n",
    "\n",
    "# kfolds = StratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=None, min_samples_split=5, min_samples_leaf=5,\n",
    "#                            random_state = 1217,  max_features='auto')\n",
    "# rf_final_test = np.zeros( test_set.shape[0] )\n",
    "\n",
    "# for ind, (trn_ind, val_ind) in tqdm_notebook( enumerate(kfolds.split(X= train_set, y = train_label)) ):\n",
    "    \n",
    "#     X_train, y_train = train_set.iloc[trn_ind], train_label[trn_ind]\n",
    "#     X_valid, y_valid = train_set.iloc[val_ind], train_label[val_ind]\n",
    "    \n",
    "#     rf.fit(X_train, y_train)\n",
    "    \n",
    "#     train_predicted_proba = rf.predict_proba(X_train)\n",
    "#     train_predicted = (train_predicted_proba [:,1] >= threshold).astype('int')\n",
    "\n",
    "#     print(accuracy_score(y_train, train_predicted), f1_score(y_train, train_predicted))\n",
    "    \n",
    "#     valid_predicted_proba = rf.predict_proba(X_valid)\n",
    "#     valid_predicted = (valid_predicted_proba [:,1] >= threshold).astype('int')\n",
    "\n",
    "#     print(accuracy_score(y_valid, valid_predicted), f1_score(y_valid, valid_predicted))\n",
    "    \n",
    "#     test_pred  = rf.predict_proba(test_set)[:,1]\n",
    "#     rf_final_test += test_pred\n",
    "    \n",
    "#     print(\"=\"*80)\n",
    "\n",
    "# test_pred_proba  = rf.predict_proba(test_set)[:,1]\n",
    "# test_pred = (test_pred >= threshold).astype('int')\n",
    "# # Counter(test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "source": [
    "## xgboost시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T06:42:38.494001Z",
     "start_time": "2021-07-31T06:42:36.591324Z"
    }
   },
   "outputs": [],
   "source": [
    "# xgb_clf = XGBClassifier(\n",
    "#     learning_rate =0.01,\n",
    "#     n_estimators=1000,\n",
    "#     min_child_weight=1,\n",
    "#     gamma=0,\n",
    "#     subsample=0.9,\n",
    "#     colsample_bytree=0.9,\n",
    "#     objective= 'binary:logistic',\n",
    "#     nthread=-1,\n",
    "#     seed=1993\n",
    "# )\n",
    "\n",
    "# kfolds = StratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "# xgb_final_test = np.zeros( test_set.shape[0] )\n",
    "\n",
    "# for ind, (trn_ind, val_ind) in tqdm_notebook( enumerate(kfolds.split(X= train_set, y = train_label)) ):\n",
    "    \n",
    "#     X_train, y_train = train_set.iloc[trn_ind], train_label[trn_ind]\n",
    "#     X_valid, y_valid = train_set.iloc[val_ind], train_label[val_ind]\n",
    "\n",
    "#     xgb_clf.fit(X_train, y_train, eval_metric='aucpr')\n",
    "\n",
    "#     train_predicted_proba = xgb_clf.predict_proba(X_train)\n",
    "#     train_predicted = (train_predicted_proba [:,1] >= threshold).astype('int')\n",
    "\n",
    "#     print(accuracy_score(y_train, train_predicted), f1_score(y_train, train_predicted))\n",
    "    \n",
    "#     valid_predicted_proba = xgb_clf.predict_proba(X_valid)\n",
    "#     valid_predicted = (valid_predicted_proba [:,1] >= threshold).astype('int')\n",
    "\n",
    "#     print(accuracy_score(y_valid, valid_predicted), f1_score(y_valid, valid_predicted))\n",
    "    \n",
    "#     test_pred  = xgb_clf.predict_proba(test_set)[:,1]\n",
    "#     xgb_final_test += test_pred\n",
    "    \n",
    "#     print(\"=\"*80)\n",
    "\n",
    "# test_pred_proba  = xgb_clf.predict_proba(test_set)[:,1]\n",
    "# test_pred = (test_pred >= threshold).astype('int')\n",
    "\n",
    "# # Counter(test_pred)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
